{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Tx58OF4d1d5a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","source":["# Downloading and extracting dataset"],"metadata":{"id":"KogC-YgEUx1a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bS0LhFI71HTv"},"outputs":[],"source":["! pip install kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dQovEA41UQ2"},"outputs":[],"source":["import os\n","\n","# Check if kaggle.json exists\n","if not os.path.exists('/root/.kaggle/kaggle.json'):\n","    from google.colab import files\n","    files.upload()\n","\n","    # Copy the uploaded file to ~/.kaggle folder\n","    ! mkdir -p ~/.kaggle\n","    ! cp kaggle.json ~/.kaggle/\n","    ! chmod 600 ~/.kaggle/kaggle.json\n","    print(\"kaggle.json file uploaded successfully.\")\n","else:\n","    print(\"kaggle.json file already exists. Skipping upload.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qckghw9H1YnH"},"outputs":[],"source":["import os\n","import shutil\n","\n","# Specify the path to the folder to be deleted\n","folder_to_delete = '/content/dataset'\n","\n","# Check if the folder exists before attempting to delete it\n","if os.path.exists(folder_to_delete):\n","    # Use shutil.rmtree() to delete the folder and its contents recursively\n","    shutil.rmtree(folder_to_delete)\n","    print(\"Folder 'dataset' and its contents have been deleted.\")\n","else:\n","    print(\"Folder 'dataset' does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFngZrSF1a-V"},"outputs":[],"source":["! kaggle datasets download -d saroz014/plant-disease"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rigtD9931dMP"},"outputs":[],"source":["! unzip plant-disease.zip"]},{"cell_type":"markdown","source":["# Data Exploration and Visualization"],"metadata":{"id":"7nzFJl7jU9ke"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxTNWxx-Yc4W"},"outputs":[],"source":["import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BqYXVdFYyuh"},"outputs":[],"source":["import shutil\n","\n","# Define the root directory of your dataset\n","root_dir = '/content/dataset/dataset/train'\n","\n","# Define the directory for the validation set\n","valid_dir = '/content/dataset/dataset/valid'\n","\n","# List all the subdirectories (classes) in the root directory\n","subdirs = [subdir for subdir in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, subdir))]\n","\n","# Iterate through each subdirectory\n","for subdir in subdirs:\n","    subdir_path = os.path.join(root_dir, subdir)\n","\n","    # List all the files (images) in the subdirectory\n","    files = [file for file in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, file))]\n","\n","    # Calculate the number of files to move to the validation set (20% of total files)\n","    num_files_valid = int(0.2 * len(files))\n","\n","    # Create the corresponding subdirectory in the validation directory if it doesn't exist\n","    valid_subdir = os.path.join(valid_dir, subdir)\n","    os.makedirs(valid_subdir, exist_ok=True)\n","\n","    # Move the files to the validation set\n","    for i in range(num_files_valid):\n","        file_to_move = files[i]\n","        src_path = os.path.join(subdir_path, file_to_move)\n","        dest_path = os.path.join(valid_subdir, file_to_move)\n","        shutil.move(src_path, dest_path)\n","        print(f'Moved {file_to_move} to {valid_subdir}')\n","\n","print('Validation data split completed.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZX1HIXJZGPa"},"outputs":[],"source":["# getting the training data from the directory, and dividing it into batches\n","train_dir = root_dir\n","train_data = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir,\n","    batch_size=32,\n","    image_size=(256,256), shuffle=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Y4KiOOxZfCq"},"outputs":[],"source":["# getting the validation data from the directory, and dividing it into batches\n","valid_data = tf.keras.preprocessing.image_dataset_from_directory(\n","    valid_dir,\n","    batch_size=32,\n","    image_size=(256,256), shuffle=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS5OH8d5ZnNR"},"outputs":[],"source":["print(f'The total nummber of classes is {len(train_data.class_names)}')\n","class_labels = train_data.class_names\n","class_labels"]},{"cell_type":"markdown","source":["# Load images and labels"],"metadata":{"id":"uByQYdK0VE6X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVzg2eZ5ZqeL"},"outputs":[],"source":["import os\n","import numpy as np\n","from keras.preprocessing import image\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0SdnBAVZ3j3"},"outputs":[],"source":["# Function to load subset of resized images and labels\n","def load_subset_resized_images_and_labels(directory, subset_size=1000, target_size=(64, 64)):\n","    images, labels = [], []\n","    classes = os.listdir(directory)\n","    print(\"Loading subset of resized images...\")\n","    for class_name in classes:\n","        class_dir = os.path.join(directory, class_name)\n","        image_files = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n","        if len(image_files) <= subset_size:\n","            selected_files = image_files\n","        else:\n","            selected_files = np.random.choice(image_files, subset_size, replace=False)\n","        for img_path in selected_files:\n","            img = image.load_img(img_path, target_size=target_size)\n","            img_array = image.img_to_array(img)\n","            images.append(img_array.flatten())  # Flatten image into a vector\n","            labels.append(classes.index(class_name))  # Assign label\n","    print(\"Subset of resized images loaded successfully.\")\n","    return np.array(images), np.array(labels)\n","\n","\n","# Load subset of resized training and validation data\n","train_images, train_labels = load_subset_resized_images_and_labels(train_dir)\n","valid_images, valid_labels = load_subset_resized_images_and_labels(valid_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn73VtQpaAxs"},"outputs":[],"source":["len(train_images),len(valid_images) # (26059, 8675)"]},{"cell_type":"markdown","source":["# Hyper parameter tuning"],"metadata":{"id":"k26XyF3-VLkm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yIJ2Qh6gxjd"},"outputs":[],"source":["# perform hyper parameter tuning on smaller subset\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","import numpy as np\n","\n","subset_size = 2000\n","subset_train_images, _, subset_train_labels, _ = train_test_split(train_images, train_labels, train_size=subset_size, stratify=train_labels)\n","\n","len(subset_train_images),len(subset_train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA6EdXSSfeFI"},"outputs":[],"source":["# perform hyper parameter tuning on smaller subset\n","# Define a smaller parameter grid for testing\n","param_grid_small = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': [0.01, 0.1, 1, 10],\n","    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n","}\n","\n","# Initialize the SVM classifier\n","svm_classifier = SVC()\n","\n","# Initialize GridSearchCV with smaller parameter grid\n","grid_search_small = GridSearchCV(estimator=svm_classifier, param_grid=param_grid_small, cv=3, scoring='accuracy')\n","\n","# Perform grid search on the subset of data\n","print(\"Performing grid search on a subset of data...\")\n","grid_search_small.fit(subset_train_images, subset_train_labels)\n","print(\"Grid search on the subset of data completed.\")\n","\n","# Get the best parameters\n","best_params_small = grid_search_small.best_params_\n","print(\"Best parameters on the subset of data:\", best_params_small)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Xjospm1TisS"},"outputs":[],"source":["# Get the CV results\n","cv_results = grid_search_small.cv_results_\n","\n","# Get the mean test scores for each parameter combination\n","mean_test_scores = cv_results['mean_test_score']\n","\n","# Get the parameters for each parameter combination\n","params = cv_results['params']\n","\n","# Sort the mean test scores in descending order\n","sorted_indices = np.argsort(mean_test_scores)[::-1]\n","\n","# Print the summary of a few best parameter combinations\n","num_combinations = 5  # Number of best combinations to display\n","print(f\"Summary of {num_combinations} best parameter combinations:\")\n","for i in range(num_combinations):\n","    idx = sorted_indices[i]\n","    print(f\"Combination {i+1}: Mean Test Score: {mean_test_scores[idx]}, Parameters: {params[idx]}\")\n"]},{"cell_type":"markdown","source":["# SVM Model Training and Evaluation\n","\n"],"metadata":{"id":"xYtExua4VVYL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"juEVNE8ng9r4"},"outputs":[],"source":["# Train the model on the full training data with the best parameters found on the subset\n","print(\"Training SVM model with best parameters...\")\n","best_svm_classifier_small = SVC(**best_params_small)\n","best_svm_classifier_small.fit(train_images, train_labels)\n","print(\"Training SVM model with best parameters completed...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xdLWO4mXknpW"},"outputs":[],"source":["# Predict on the validation set\n","val_predictions_svm_tuned = best_svm_classifier_small.predict(valid_images)\n","\n","# Evaluate the tuned model\n","print('Validation set accuracy :', accuracy_score(valid_labels, val_predictions_svm_tuned))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JHfNtc0B6eqH"},"outputs":[],"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Generate confusion matrix\n","conf_matrix = confusion_matrix(valid_labels, val_predictions_svm_tuned)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","\n","print(\"\\n\")\n","\n","# Generate classification report\n","class_report = classification_report(valid_labels, val_predictions_svm_tuned)\n","print(\"Classification Report:\")\n","print(class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tf2sT6kBCU-b"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()"]},{"cell_type":"markdown","source":["# Evaluating with test data"],"metadata":{"id":"w2d3u0KJVbae"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8h8x0DF3Ci48"},"outputs":[],"source":["# getting the test data from the directory, and dividing it into batches\n","test_dir = '/content/dataset/dataset/test'\n","\n","test_data = tf.keras.preprocessing.image_dataset_from_directory(\n","    test_dir,\n","    batch_size=32,\n","    image_size=(256,256), shuffle=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DL-jl3vdDRoa"},"outputs":[],"source":["test_images, test_labels = load_subset_resized_images_and_labels(test_dir)\n","\n","len(test_images),len(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C3YVqKp9Dmhx"},"outputs":[],"source":["# Predict on the test set\n","test_predictions_svm_tuned = best_svm_classifier_small.predict(test_images)\n","\n","# Evaluate the tuned model\n","print('Test set accuracy :', accuracy_score(test_labels, test_predictions_svm_tuned))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RwhJ_B8sD63g"},"outputs":[],"source":["# Generate confusion matrix\n","test_conf_matrix = confusion_matrix(valid_labels, val_predictions_svm_tuned)\n","print(\"Confusion Matrix:\")\n","print(test_conf_matrix)\n","\n","print(\"\\n\")\n","\n","# Generate classification report\n","test_class_report = classification_report(valid_labels, val_predictions_svm_tuned)\n","print(\"Classification Report:\")\n","print(test_class_report)\n","\n","print(\"\\n\")\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["KogC-YgEUx1a","7nzFJl7jU9ke","uByQYdK0VE6X","k26XyF3-VLkm","xYtExua4VVYL","w2d3u0KJVbae"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}